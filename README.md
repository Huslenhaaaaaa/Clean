                                           General 
                                             
My repository automatically scrapes all the ad information relating to buying or renting apartments from Unegui.mn
It automatically scrapes and updates the information so overtime it will keep gathering more and more data 
which I think would be useful for maybe developing a perdiction model or for future uses.

Then based off that data is a Streamlit dashboard that shows off the useful parts of the information for users who 
are looking to move and wish to see thier options. 

                   https://nbuyhxjfjeiyf9pekhyhan.streamlit.app/


Repository Structure
├── .github/workflows/
│   └── scrape_rental.yml   
├── unegui_data/            
├── Analysis.ipynb          
├── Model.ipynb             
├── Scraper.py             
├── apartment_price_prediction_model.pkl 
├── app.py                 
└── requirements.txt        
